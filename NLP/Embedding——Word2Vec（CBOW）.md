# Embedding——Word2Vec（CBOW）

### CBOW

对于Embedding的实现有一种比较成熟的工具就是Word2Vec，这一篇文章主要对向量化中的一种方法进行展开——CBOW

#### 单个上下文词汇模型

对于CBOW，最简单的模型就是根据上下文的一个词进行预测，也就是我们所谓的单个单词上下文，网络模型如下：

![cbow](F:\markdown\cbow.png)

可以看到输入向量是一个具有V个元素的one-hot向量，它代表了我们的词汇量总共有V个，而只有上下文单词对应的词汇表中的位置是1，其它都是0，，接下来全连接一个隐藏层，然后全连接一个输出层。为了更好地理解这个模型，我们简化上面的模型为下面的表格：

![cbow1](F:\markdown\cbow1.png)

输入层和隐藏层之间的权重可以用一个V*N的矩阵表示，因此给定一个上下文的词汇，对于这个权重矩阵来说，只有第k（k为上下文词汇对应的位置）行是有效的，我们相当于复制了这一行出来：
$$
h=W^T_k=V_i
$$
对于隐藏层的激活函数，一般视为简单地线性函数，因此我们可以直接将输入的加权和传递到下一层，而在隐藏层与输出层之间，又有一个不同的权重矩阵，大小是N*V的矩阵，使用这些权重，就可以计算每一个单词的权重：
$$
u_j=v_j^Th
$$
最后我们使用softmax，一个log线性分类器模型，得到每一个单词的后验概率：
$$
p(w_i|w_I)=y_i=\frac{exp(v_{w_j}^Tv_{wI})}{\sum_{j=1}^Vexp(v_{w_j}^Tv_{wI})}
$$
我们的目的就是最大化这个后验概率，最小化损失函数，因此对于隐藏层到输出层的权重更新，因此有：
$$
max \space p(w_0|w_1)=max\space y_j=max\space log\space y_j\\
=max\space log \space y_j\\
=u_j-log\sum_jexp(u_j)=-E
$$
对上面的损失函数求导，使用随机梯度下降方法进行优化求解：
$$
v_{w_j}^{new}=v_{w_j}^{old}-\eta e_j h
$$
这个更新等式需要我们将词汇表遍历一遍，检查输出的概率与期望输出，如果高估，就需要减少隐藏层权重比例，如果低估就要增加比例，使得相似两者不断靠近。但是这里衡量远近的标准是内积，不是欧氏距离。

使用同样的方法对上一层的权重进行更新：
$$
v_{wI}^{new}=v_{wI}^{old}-\eta EH^T
$$


#### 多个上次文词汇模型

![cbow2](F:\markdown\cbow2.png)

因为添加了多层的上下文输入，所以不能像单词语上下文这样，直接复制上下文单词的输入向量，而是取所有上下文单词输入向量的平均值，然后将输入和隐藏层权重矩阵和这个平均值的乘积作为输出：
$$
h=\frac{1}{C}W^T(x_1+x_2+x_3+...+x_c)
$$
同样地隐藏层到输出层的权重更新一样：
$$
v_{w_j}^{new}=v_{w_j}^{old}-\eta e_j h
$$
输入层到隐藏层的权重更新也是与之前的类似：
$$
v_{wI}^{new}=v_{wI}^{old}-\frac{1}{C}\eta EH^T
$$

### 参考：

https://blog.csdn.net/hsiffish/article/details/78673924





